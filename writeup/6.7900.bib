
@misc{liptonMythosModelInterpretability2017,
	title = {The {Mythos} of {Model} {Interpretability}},
	url = {http://arxiv.org/abs/1606.03490},
	abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Lipton, Zachary C.},
	month = mar,
	year = {2017},
	note = {arXiv:1606.03490 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/katiechen/Zotero/storage/PGQGGG48/1606.html:text/html;Full Text PDF:/Users/katiechen/Zotero/storage/JASGQCNU/Lipton - 2017 - The Mythos of Model Interpretability.pdf:application/pdf},
}

@inproceedings{lundbergUnifiedApproachInterpreting2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	urldate = {2023-11-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lundberg, Scott M and Lee, Su-In},
	year = {2017},
	file = {Full Text PDF:/Users/katiechen/Zotero/storage/NJYUEF32/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf:application/pdf},
}

@misc{ribeiroWhyShouldTrust2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://arxiv.org/abs/1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {arXiv:1602.04938 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/katiechen/Zotero/storage/VV3LC2KI/1602.html:text/html;Full Text PDF:/Users/katiechen/Zotero/storage/WG9XXNHZ/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf},
}

@misc{norrenbrockTakeInterpretableImage2023,
	title = {Take 5: {Interpretable} {Image} {Classification} with a {Handful} of {Features}},
	shorttitle = {Take 5},
	url = {http://arxiv.org/abs/2303.13166},
	abstract = {Deep Neural Networks use thousands of mostly incomprehensible features to identify a single class, a decision no human can follow. We propose an interpretable sparse and low dimensional final decision layer in a deep neural network with measurable aspects of interpretability and demonstrate it on fine-grained image classification. We argue that a human can only understand the decision of a machine learning model, if the features are interpretable and only very few of them are used for a single decision. For that matter, the final layer has to be sparse and, to make interpreting the features feasible, low dimensional. We call a model with a Sparse Low-Dimensional Decision SLDD-Model. We show that a SLDD-Model is easier to interpret locally and globally than a dense high-dimensional decision layer while being able to maintain competitive accuracy. Additionally, we propose a loss function that improves a model's feature diversity and accuracy. Our more interpretable SLDD-Model only uses 5 out of just 50 features per class, while maintaining 97\% to 100\% of the accuracy on four common benchmark datasets compared to the baseline model with 2048 features.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Norrenbrock, Thomas and Rudolph, Marco and Rosenhahn, Bodo},
	month = aug,
	year = {2023},
	note = {arXiv:2303.13166 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.4},
	file = {arXiv.org Snapshot:/Users/katiechen/Zotero/storage/LWVNGMIL/2303.html:text/html;Full Text PDF:/Users/katiechen/Zotero/storage/CDQMUCBI/Norrenbrock et al. - 2023 - Take 5 Interpretable Image Classification with a .pdf:application/pdf},
}

@misc{rymarczykInterpretableImageClassification2022,
	title = {Interpretable {Image} {Classification} with {Differentiable} {Prototypes} {Assignment}},
	url = {http://arxiv.org/abs/2112.02902},
	doi = {10.48550/arXiv.2112.02902},
	abstract = {We introduce ProtoPool, an interpretable image classification model with a pool of prototypes shared by the classes. The training is more straightforward than in the existing methods because it does not require the pruning stage. It is obtained by introducing a fully differentiable assignment of prototypes to particular classes. Moreover, we introduce a novel focal similarity function to focus the model on the rare foreground features. We show that ProtoPool obtains state-of-the-art accuracy on the CUB-200-2011 and the Stanford Cars datasets, substantially reducing the number of prototypes. We provide a theoretical analysis of the method and a user study to show that our prototypes are more distinctive than those obtained with competitive methods.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Rymarczyk, Dawid and Struski, Łukasz and Górszczak, Michał and Lewandowska, Koryna and Tabor, Jacek and Zieliński, Bartosz},
	month = sep,
	year = {2022},
	note = {arXiv:2112.02902 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/katiechen/Zotero/storage/7EXGZU4S/Rymarczyk et al. - 2022 - Interpretable Image Classification with Differenti.pdf:application/pdf;arXiv.org Snapshot:/Users/katiechen/Zotero/storage/36Q3ZIKR/2112.html:text/html},
}

@article{zhangImprovingInterpretabilityCNN2020,
	title = {Improving {Interpretability} of {CNN} {Models} {Using} {Non}-{Negative} {Concept} {Activation} {Vectors}},
	url = {https://www.semanticscholar.org/paper/Improving-Interpretability-of-CNN-Models-Using-Zhang-Madumal/927e0f987e9163eb424510f300516773816076a0},
	abstract = {Convolutional neural network (CNN) models for computer vision are powerful but lack explainability in their most basic form. This deficiency remains a key challenge when applying CNNs in important domains. Recent work for explanations through feature importance of approximate linear models has moved from input-level features (pixels or segments) to features from mid-layer feature maps in the guise of concept activation vectors (CAVs). CAVs contain concept-level information and could be learnt via Clustering. In this work, we rethink the ACE algorithm of Ghorbani et{\textasciitilde}al., proposing an alternative concept-based explanation framework. Based on the requirements of fidelity (approximate models) and interpretability (being meaningful to people), we design measurements and evaluate a range of dimensionality reduction methods for alignment with our framework. We find that non-negative concept activation vectors from non-negative matrix factorization provide superior performance in interpretability and fidelity based on computational and human subject experiments. Our framework provides both local and global concept-level explanations for pre-trained CNN models.},
	urldate = {2023-11-01},
	journal = {ArXiv},
	author = {Zhang, Ruihan and Madumal, Prashan and Miller, Tim and Ehinger, Krista A. and Rubinstein, Benjamin I. P.},
	month = jun,
	year = {2020},
}

@article{liuVisualSaliencyTransformer2021,
	title = {Visual {Saliency} {Transformer}},
	url = {https://www.semanticscholar.org/paper/Visual-Saliency-Transformer-Liu-Zhang/751b71158b7dcd2a7949e72a6ad8fb13657a401c},
	doi = {10.1109/ICCV48922.2021.00468},
	abstract = {This work develops a novel unified model based on a pure transformer, namely, Visual Saliency Transformer, for both RGB and RGB-D salient object detection (SOD), which takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is available at https://github.com/nnizhang/VST.},
	language = {en},
	urldate = {2023-11-01},
	journal = {IEEE International Conference on Computer Vision},
	author = {Liu, Nian and Zhang, Ni and Wan, Kaiyuan and Han, Junwei and Shao, Ling},
	year = {2021},
	file = {Snapshot:/Users/katiechen/Zotero/storage/HBS8K955/751b71158b7dcd2a7949e72a6ad8fb13657a401c.html:text/html;Submitted Version:/Users/katiechen/Zotero/storage/788WTJCU/Liu et al. - 2021 - Visual Saliency Transformer.pdf:application/pdf},
}

@misc{liaoArtBenchDatasetBenchmarking2022a,
	title = {The {ArtBench} {Dataset}: {Benchmarking} {Generative} {Models} with {Artworks}},
	shorttitle = {The {ArtBench} {Dataset}},
	url = {http://arxiv.org/abs/2206.11404},
	abstract = {We introduce ArtBench-10, the first class-balanced, high-quality, cleanly annotated, and standardized dataset for benchmarking artwork generation. It comprises 60,000 images of artwork from 10 distinctive artistic styles, with 5,000 training images and 1,000 testing images per style. ArtBench-10 has several advantages over previous artwork datasets. Firstly, it is class-balanced while most previous artwork datasets suffer from the long tail class distributions. Secondly, the images are of high quality with clean annotations. Thirdly, ArtBench-10 is created with standardized data collection, annotation, filtering, and preprocessing procedures. We provide three versions of the dataset with different resolutions (\$32{\textbackslash}times32\$, \$256{\textbackslash}times256\$, and original image size), formatted in a way that is easy to be incorporated by popular machine learning frameworks. We also conduct extensive benchmarking experiments using representative image synthesis models with ArtBench-10 and present in-depth analysis. The dataset is available at https://github.com/liaopeiyuan/artbench under a Fair Use license.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Liao, Peiyuan and Li, Xiuyu and Liu, Xihui and Keutzer, Kurt},
	month = jun,
	year = {2022},
	note = {arXiv:2206.11404 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/katiechen/Zotero/storage/JWXT8LTD/2206.html:text/html;Full Text PDF:/Users/katiechen/Zotero/storage/FMBVQ4J4/Liao et al. - 2022 - The ArtBench Dataset Benchmarking Generative Mode.pdf:application/pdf},
}

@misc{radfordLearningTransferableVisual2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/katiechen/Zotero/storage/PGKL9D9B/2103.html:text/html;Full Text PDF:/Users/katiechen/Zotero/storage/2A6MPWZG/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf},
}

@article{sharmaAnalysisConvolutionalNeural2018,
	series = {International {Conference} on {Computational} {Intelligence} and {Data} {Science}},
	title = {An {Analysis} {Of} {Convolutional} {Neural} {Networks} {For} {Image} {Classification}},
	volume = {132},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050918309335},
	doi = {10.1016/j.procs.2018.05.198},
	abstract = {This paper presents an empirical analysis of theperformance of popular convolutional neural networks (CNNs) for identifying objects in real time video feeds. The most popular convolution neural networks for object detection and object category classification from images are Alex Nets, GoogLeNet, and ResNet50. A variety of image data sets are available to test the performance of different types of CNN’s. The commonly found benchmark datasets for evaluating the performance of a convolutional neural network are anImageNet dataset, and CIFAR10, CIFAR100, and MNIST image data sets. This study focuses on analyzing the performance of three popular networks: Alex Net, GoogLeNet, and ResNet50. We have taken three most popular data sets ImageNet, CIFAR10, and CIFAR100 for our study, since, testing the performance of a network on a single data set does not reveal its true capability and limitations. It must be noted that videos are not used as a training dataset, they are used as testing datasets. Our analysis shows that GoogLeNet and ResNet50 are able to recognize objects with better precision compared to Alex Net. Moreover, theperformance of trained CNN’s vary substantially across different categories of objects and we, therefore, will discuss the possible reasons for this.},
	urldate = {2023-11-15},
	journal = {Procedia Computer Science},
	author = {Sharma, Neha and Jain, Vibhor and Mishra, Anju},
	month = jan,
	year = {2018},
	keywords = {CNN, Deep Learning, Neural network, Object classification, Object detection},
	pages = {377--384},
	file = {ScienceDirect Full Text PDF:/Users/katiechen/Zotero/storage/N8U3RUJS/Sharma et al. - 2018 - An Analysis Of Convolutional Neural Networks For I.pdf:application/pdf;ScienceDirect Snapshot:/Users/katiechen/Zotero/storage/PYBQAQ33/S1877050918309335.html:text/html},
}

@misc{alainUnderstandingIntermediateLayers2018,
	title = {Understanding intermediate layers using linear classifier probes},
	url = {http://arxiv.org/abs/1610.01644},
	doi = {10.48550/arXiv.1610.01644},
	abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
	urldate = {2023-11-15},
	publisher = {arXiv},
	author = {Alain, Guillaume and Bengio, Yoshua},
	month = nov,
	year = {2018},
	note = {arXiv:1610.01644 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/katiechen/Zotero/storage/X76DFH57/Alain and Bengio - 2018 - Understanding intermediate layers using linear cla.pdf:application/pdf;arXiv.org Snapshot:/Users/katiechen/Zotero/storage/4RL68ETD/1610.html:text/html},
}

@article{xuEmpiricalEvaluationRectified2015,
	title = {Empirical {Evaluation} of {Rectified} {Activations} in {Convolutional} {Network}},
	url = {https://www.semanticscholar.org/paper/Empirical-Evaluation-of-Rectified-Activations-in-Xu-Wang/adf3b591281688b7e71b254ab931b2aa39b4b59f},
	abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68{\textbackslash}\% accuracy on CIFAR-100 test set without multiple test or ensemble.},
	urldate = {2023-11-15},
	journal = {ArXiv},
	author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
	month = may,
	year = {2015},
	file = {Full Text PDF:/Users/katiechen/Zotero/storage/WXRSBQRQ/Xu et al. - 2015 - Empirical Evaluation of Rectified Activations in C.pdf:application/pdf},
}

@misc{gatysNeuralAlgorithmArtistic2015,
	title = {A {Neural} {Algorithm} of {Artistic} {Style}},
	url = {http://arxiv.org/abs/1508.06576},
	abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
	urldate = {2023-11-15},
	publisher = {arXiv},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	month = sep,
	year = {2015},
	note = {arXiv:1508.06576 [cs, q-bio]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/katiechen/Zotero/storage/XEMYLPBP/1508.html:text/html;Full Text PDF:/Users/katiechen/Zotero/storage/FFMSENZD/Gatys et al. - 2015 - A Neural Algorithm of Artistic Style.pdf:application/pdf},
}

@misc{gatysTextureSynthesisUsing2015,
	title = {Texture {Synthesis} {Using} {Convolutional} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1505.07376v3},
	abstract = {Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.},
	language = {en},
	urldate = {2023-11-16},
	journal = {arXiv.org},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	month = may,
	year = {2015},
	file = {Full Text PDF:/Users/katiechen/Zotero/storage/MXFGWL8B/Gatys et al. - 2015 - Texture Synthesis Using Convolutional Neural Netwo.pdf:application/pdf},
}

@misc{simonyanVeryDeepConvolutional2014,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {https://arxiv.org/abs/1409.1556v6},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	language = {en},
	urldate = {2023-11-16},
	journal = {arXiv.org},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = sep,
	year = {2014},
	file = {Full Text PDF:/Users/katiechen/Zotero/storage/F7Y4CFM2/Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf},
}

@inproceedings{huImageStyleTransfer2020,
	title = {Image {Style} {Transfer} based on {Generative} {Adversarial} {Network}},
	volume = {1},
	url = {https://ieeexplore.ieee.org/document/9084750},
	doi = {10.1109/ITNEC48623.2020.9084750},
	abstract = {The traditional style transfer based on GAN model is limited by paired images. CycleGAN solves this problem effectively, but its structure is complex and training time-consuming. This paper proposed a style transfer model based on generative adversarial network, which abandons the redundant structure of two GAN models trained by CycleGAN, and trains only one generator and one discriminator without pairing image samples. And the semantic content between the input image and the generated image is constrained by the VGG network feature map. In order to accelerate the convergence of the model, a pretraining stage is introduced. The experimental results show that the proposed model is effective than CycleGAN, and outperforms state-of-the-art methods.},
	urldate = {2023-11-17},
	booktitle = {2020 {IEEE} 4th {Information} {Technology}, {Networking}, {Electronic} and {Automation} {Control} {Conference} ({ITNEC})},
	author = {Hu, Chan and Ding, Youdong and Li, Yuhang},
	month = jun,
	year = {2020},
	pages = {2098--2102},
	file = {IEEE Xplore Abstract Record:/Users/katiechen/Zotero/storage/92RM7AQD/9084750.html:text/html;IEEE Xplore Full Text PDF:/Users/katiechen/Zotero/storage/KYWPKJFH/Hu et al. - 2020 - Image Style Transfer based on Generative Adversari.pdf:application/pdf},
}

@misc{karrasStyleBasedGeneratorArchitecture2019,
	title = {A {Style}-{Based} {Generator} {Architecture} for {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1812.04948},
	doi = {10.48550/arXiv.1812.04948},
	abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
	urldate = {2023-11-17},
	publisher = {arXiv},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	month = mar,
	year = {2019},
	note = {arXiv:1812.04948 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/katiechen/Zotero/storage/8NBYHSHE/Karras et al. - 2019 - A Style-Based Generator Architecture for Generativ.pdf:application/pdf;arXiv.org Snapshot:/Users/katiechen/Zotero/storage/RW6PQARC/1812.html:text/html},
}

@misc{liuMultipleStyleTransfer2021,
	title = {Multiple {Style} {Transfer} via {Variational} {AutoEncoder}},
	url = {http://arxiv.org/abs/2110.07375},
	doi = {10.48550/arXiv.2110.07375},
	abstract = {Modern works on style transfer focus on transferring style from a single image. Recently, some approaches study multiple style transfer; these, however, are either too slow or fail to mix multiple styles. We propose ST-VAE, a Variational AutoEncoder for latent space-based style transfer. It performs multiple style transfer by projecting nonlinear styles to a linear latent space, enabling to merge styles via linear interpolation before transferring the new style to the content image. To evaluate ST-VAE, we experiment on COCO for single and multiple style transfer. We also present a case study revealing that ST-VAE outperforms other methods while being faster, flexible, and setting a new path for multiple style transfer.},
	urldate = {2023-11-17},
	publisher = {arXiv},
	author = {Liu, Zhi-Song and Kalogeiton, Vicky and Cani, Marie-Paule},
	month = oct,
	year = {2021},
	note = {arXiv:2110.07375 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/katiechen/Zotero/storage/LH4NRWES/Liu et al. - 2021 - Multiple Style Transfer via Variational AutoEncode.pdf:application/pdf;arXiv.org Snapshot:/Users/katiechen/Zotero/storage/33Q33X2E/2110.html:text/html},
}

@misc{yosinskiUnderstandingNeuralNetworks2015a,
	title = {Understanding {Neural} {Networks} {Through} {Deep} {Visualization}},
	url = {http://arxiv.org/abs/1506.06579},
	abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
	urldate = {2023-11-17},
	publisher = {arXiv},
	author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
	month = jun,
	year = {2015},
	note = {arXiv:1506.06579 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/katiechen/Zotero/storage/U5WXEV7Y/1506.html:text/html;Full Text PDF:/Users/katiechen/Zotero/storage/FEHAUBPR/Yosinski et al. - 2015 - Understanding Neural Networks Through Deep Visuali.pdf:application/pdf},
}

@misc{bauNetworkDissectionQuantifying2017,
	title = {Network {Dissection}: {Quantifying} {Interpretability} of {Deep} {Visual} {Representations}},
	shorttitle = {Network {Dissection}},
	url = {http://arxiv.org/abs/1704.05796},
	doi = {10.48550/arXiv.1704.05796},
	abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
	urldate = {2023-11-17},
	publisher = {arXiv},
	author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
	month = apr,
	year = {2017},
	note = {arXiv:1704.05796 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, I.2.10},
	file = {arXiv Fulltext PDF:/Users/katiechen/Zotero/storage/ZSG2TC8E/Bau et al. - 2017 - Network Dissection Quantifying Interpretability o.pdf:application/pdf;arXiv.org Snapshot:/Users/katiechen/Zotero/storage/QMKW7XZZ/1704.html:text/html},
}

@misc{zeilerVisualizingUnderstandingConvolutional2013,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	doi = {10.48550/arXiv.1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	urldate = {2023-11-17},
	publisher = {arXiv},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = nov,
	year = {2013},
	note = {arXiv:1311.2901 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/katiechen/Zotero/storage/IIQTGQIK/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf;arXiv.org Snapshot:/Users/katiechen/Zotero/storage/4T644A2F/1311.html:text/html},
}

@misc{johnsonNeuralstyle2023,
	title = {neural-style},
	copyright = {MIT},
	url = {https://github.com/jcjohnson/neural-style},
	abstract = {Torch implementation of neural style algorithm},
	urldate = {2023-11-17},
	author = {Johnson, Justin},
	month = nov,
	year = {2023},
	note = {original-date: 2015-09-01T04:55:14Z},
}

@article{harlevExploringLearnedRepresentations2023,
	title = {Exploring {Learned} {Representations} of {Neural} {Networks} with {Principal} {Component} {Analysis}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2309.15328},
	doi = {10.48550/ARXIV.2309.15328},
	abstract = {Understanding feature representation for deep neural networks (DNNs) remains an open question within the general field of explainable AI. We use principal component analysis (PCA) to study the performance of a k-nearest neighbors classifier (k-NN), nearest class-centers classifier (NCC), and support vector machines on the learned layer-wise representations of a ResNet-18 trained on CIFAR-10. We show that in certain layers, as little as 20\% of the intermediate feature-space variance is necessary for high-accuracy classification and that across all layers, the first {\textasciitilde}100 PCs completely determine the performance of the k-NN and NCC classifiers. We relate our findings to neural collapse and provide partial evidence for the related phenomenon of intermediate neural collapse. Our preliminary work provides three distinct yet interpretable surrogate models for feature representation with an affine linear model the best performing. We also show that leveraging several surrogate models affords us a clever method to estimate where neural collapse may initially occur within the DNN.},
	urldate = {2023-11-27},
	author = {Harlev, Amit and Engel, Andrew and Stinis, Panos and Chiang, Tony},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{heDeepResidualLearning2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2023-11-30},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/katiechen/Zotero/storage/B75YGGIF/1512.html:text/html;Full Text PDF:/Users/katiechen/Zotero/storage/DUEUL89E/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@misc{liYourDiffusionModel,
	title = {Your {Diffusion} {Model} is {Secretly} a {Zero}-{Shot} {Classifier}},
	language = {en},
	author = {Li, Alexander C and Prabhudesai, Mihir and Duggal, Shivam and Brown, Ellis and Pathak, Deepak},
	file = {Li et al. - Your Diffusion Model is Secretly a Zero-Shot Class.pdf:/Users/katiechen/Zotero/storage/XRWCKRXQ/Li et al. - Your Diffusion Model is Secretly a Zero-Shot Class.pdf:application/pdf},
       urldate = {2023-11-15},
}


@misc{Elgammal,
	title = {The Shape of Art History in the Eyes of the Machine},
	url = {https://arxiv.org/abs/1801.07729},
	abstract = {How does the machine classify styles in art? And how does it relate to art historians' methods for analyzing style? Several studies have shown the ability of the machine to learn and predict style categories, such as Renaissance, Baroque, Impressionism, etc., from images of paintings. This implies that the machine can learn an internal representation encoding discriminative features through its visual analysis. However, such a representation is not necessarily interpretable. We conducted a comprehensive study of several of the state-of-the-art convolutional neural networks applied to the task of style classification on 77K images of paintings, and analyzed the learned representation through correlation analysis with concepts derived from art history. Surprisingly, the networks could place the works of art in a smooth temporal arrangement mainly based on learning style labels, without any a priori knowledge of time of creation, the historical time and context of styles, or relations between styles. The learned representations showed that there are few underlying factors that explain the visual variations of style in art. Some of these factors were found to correlate with style patterns suggested by Heinrich Wölfflin (1846-1945). The learned representations also consistently highlighted certain artists as the extreme distinctive representative of their styles, which quantitatively confirms art historian observations.},
	urldate = {2023-12-15},
	publisher = {arXiv},
	author = {Ahmed Elgammal and Marian Mazzone and Bingchen Liu and Diana Kim and Mohamed Elhoseiny},
	month = feb,
	year = {2018},
}

@inproceedings{huangArbitraryStyleTransfer2017a,
	address = {Venice},
	title = {Arbitrary {Style} {Transfer} in {Real}-{Time} with {Adaptive} {Instance} {Normalization}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237429/},
	doi = {10.1109/ICCV.2017.167},
	abstract = {Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a ﬁxed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the ﬁrst time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-deﬁned set of styles. In addition, our approach allows ﬂexible user controls such as content-style trade-off, style interpolation, color \& spatial controls, all using a single feed-forward neural network.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Huang, Xun and Belongie, Serge},
	month = oct,
	year = {2017},
	pages = {1510--1519},
	file = {Huang and Belongie - 2017 - Arbitrary Style Transfer in Real-Time with Adaptiv.pdf:/Users/katiechen/Zotero/storage/5WXI37SZ/Huang and Belongie - 2017 - Arbitrary Style Transfer in Real-Time with Adaptiv.pdf:application/pdf},
}

@misc{arbitrarynst,
	title = {Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization},
	url = {https://arxiv.org/abs/1703.06868},
	abstract = {Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network.},
	urldate = {2023-12-15},
	publisher = {arXiv},
	author = {Xun Huang and Serge Belongie},
	month = mar,
	year = {2017},
}
